from typing import Tuple

import torch
from torch.distributions import normal
from torch.nn.utils.parametrizations import spectral_norm
import torch.nn as nn
from common import LBlock, AttentionLayer, DBlock
import einops

##TODO: Now only generate one sample, and not one batch
class LatentConditionStack(nn.Module):
    def __init__(self, in_shape):
        """
        in_shape dims -> (8, 8, 8) -> (C, H, W)
        """
        super().__init__()

        self.in_shape = in_shape
        self.in_channel = in_shape[0]

        self.dist = normal.Normal(loc=0.0, scale=1.0)

        self.conv3x3 = spectral_norm(
            nn.Conv2d(
                in_channels=self.in_channel,
                out_channels=self.in_channel,
                kernel_size=(3, 3),
                padding=1
            )
        )

        self.l1 = LBlock(self.in_channel, 24)
        self.l2 = LBlock(24, 48)
        self.l3 = LBlock(48, 192)
        self.attn = AttentionLayer(192, 192)
        self.l4 = LBlock(192, 768)

    def forward(self, batch_size=1):
        target_shape = [batch_size] + [*self.in_shape]
        z = self.dist.sample(target_shape)
        
        ## first conv
        z = self.conv3x3(z)

        ## Lblock
        z = self.l1(z)
        z = self.l2(z)
        z = self.l3(z)
        z = self.attn(z)

        z = self.l4(z)

        return z

class ContextConditionStack(nn.Module):
    def __init__(self, in_channels: int = 1,
                 final_channel: int = 384):
        """
        """
        super().__init__()
        self.in_channels = in_channels
        
        self.space_to_depth = nn.PixelUnshuffle(downscale_factor=2)

        in_c = in_channels
        ## different scales channels
        chs = [4*in_c, 24*in_c, 48*in_c, 96*in_c, 192*in_c]
        self.Dlist = nn.ModuleList()
        self.convList = nn.ModuleList()
        for i in range(len(chs)-1):
            self.Dlist.append(
                DBlock(in_channel=chs[i],
                       out_channel=chs[i+1],
                       apply_relu=True, apply_down=True)
            )

            self.convList.append(
                nn.Conv2d(in_channels=4 * chs[i+1],
                          out_channels=4 * chs[i+1] // 2,
                          kernel_size=(3, 3),
                          padding=1)
            )

        ## ReLU
        self.relu = nn.ReLU()

    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        ## input dims -> (N, D, C, H, W)
        x = self.space_to_depth(x)
        steps = x.shape[1]

        ## different feature index represent different scale
        features = [[] for i in range(steps)]

        for st in range(steps):
            in_x = x[:, st, :, :, :]
            ## in_x -> (N, C, H, W)
            for scale in range(4):
                in_x = self.Dlist[scale](in_x)
                features[scale].append(in_x)
        
        out_scale = []
        for i, cc in enumerate(self.convList):
            ## after stacking, dims -> (N, D, C, H, W)
            stacked = self._mixing_layer(torch.stack(features[i], dim=1))
            out = self.relu(cc(stacked))
            out_scale.append(out)

        return out_scale

    def _mixing_layer(self, x):
        # conver from (N, D, C, H, W) -> (N, D*C, H, W)
        # Then apply Conv2d
        stacked = einops.rearrange(x, "b t c h w -> b (c t) h w")
        
        return stacked


if __name__ == '__main__':

    ## (N, D, C, H, W)
    input_tensor = torch.rand(10, 4, 1, 256, 256)

    # (N, C, H, W)
    LatentStack = LatentConditionStack((8, 8, 8))
    zlatent = LatentStack()
    #print(zlatent.shape)
    
    ## ContextConditionStack test
    ContextStack = ContextConditionStack()
    context_inits = ContextStack(input_tensor)

    for i in context_inits:
        print(i.shape)


